{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e611d317",
   "metadata": {},
   "source": [
    "# Building a Retrieval-Augmented Generation (RAG) System\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) system using:\n",
    "- LangChain for document processing and vector operations\n",
    "- FAISS for efficient similarity search\n",
    "- Ollama for local LLM inference\n",
    "- HuggingFace embeddings for text vectorization\n",
    "\n",
    "We'll build a system that can answer questions about PDF documents by:\n",
    "1. Loading and processing PDFs\n",
    "2. Creating vector embeddings from the text\n",
    "3. Performing similarity search for relevant content\n",
    "4. Generating contextual answers using a local LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a03a6",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Import Libraries\n",
    "\n",
    "First, we need to install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec55a92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install langchain faiss-cpu pypdf2 ollama sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7f977",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb561664",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12dac79",
   "metadata": {},
   "source": [
    "Now, let's import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439bf5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import ollama\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from IPython.display import Markdown, display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c628bfd5",
   "metadata": {},
   "source": [
    "## 2. PDF Loading and Text Extraction\n",
    "\n",
    "We'll create a function to load PDF files and extract text from them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b18e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Load a PDF file and extract text from it.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text from the PDF\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
    "    \n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "        text = \"\"\n",
    "        num_pages = len(reader.pages)\n",
    "        \n",
    "        print(f\"Processing {num_pages} pages...\")\n",
    "        \n",
    "        for i, page in enumerate(reader.pages):\n",
    "            page_text = page.extract_text()\n",
    "            text += page_text + \"\\n\"\n",
    "            # Print progress\n",
    "            if (i+1) % 5 == 0 or i+1 == num_pages:\n",
    "                print(f\"Processed {i+1}/{num_pages} pages\")\n",
    "                \n",
    "    print(f\"Extracted {len(text)} characters of text.\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc15788",
   "metadata": {},
   "source": [
    "Let's test our PDF loading function with a sample PDF. For this example, we'll use the annual report PDF mentioned in the original code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c205d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the PDF file\n",
    "pdf_path = \"../annual-report-2024.pdf\"\n",
    "\n",
    "try:\n",
    "    # Load and process PDF\n",
    "    pdf_text = load_pdf(pdf_path)\n",
    "    \n",
    "    # Display a sample of the extracted text\n",
    "    print(\"\\nSample of extracted text (first 500 characters):\")\n",
    "    print(\"=\"*80)\n",
    "    print(pdf_text[:500])\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Total text length: {len(pdf_text)} characters\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {pdf_path}\")\n",
    "    print(\"For this example, please place a PDF file at the specified path or update the path.\")\n",
    "    # Create a placeholder text for demonstration purposes\n",
    "    pdf_text = \"This is a sample text that simulates content from a PDF file. \" * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd40a6c3",
   "metadata": {},
   "source": [
    "## 3. Text Chunking and Vector Store Creation\n",
    "\n",
    "For effective retrieval, we need to split the text into manageable chunks and create embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252435e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into chunks and create a vector store.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to process\n",
    "        chunk_size (int): Size of each chunk\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        \n",
    "    Returns:\n",
    "        FAISS: Vector store object\n",
    "    \"\"\"\n",
    "    # Split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    chunking_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Split text into {len(chunks)} chunks (took {chunking_time:.2f} seconds)\")\n",
    "    \n",
    "    # Create embeddings\n",
    "    print(\"Initializing embedding model...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    \n",
    "    # Create vector store\n",
    "    print(\"Creating vector store...\")\n",
    "    start_time = time.time()\n",
    "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "    vectorization_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Vector store created with {len(chunks)} vectors (took {vectorization_time:.2f} seconds)\")\n",
    "    \n",
    "    return vector_store, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00744cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector store from the PDF text\n",
    "vector_store, chunks = create_vector_store(pdf_text)\n",
    "\n",
    "# Display some statistics about the chunks\n",
    "chunk_lengths = [len(chunk) for chunk in chunks]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(chunk_lengths, bins=20)\n",
    "plt.title('Distribution of Chunk Lengths')\n",
    "plt.xlabel('Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average chunk length: {np.mean(chunk_lengths):.1f} characters\")\n",
    "print(f\"Shortest chunk: {min(chunk_lengths)} characters\")\n",
    "print(f\"Longest chunk: {max(chunk_lengths)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835ae46",
   "metadata": {},
   "source": [
    "Let's examine a few chunks to understand what our text looks like after chunking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb962514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display a few example chunks\n",
    "# for i in range(min(3, len(chunks))):\n",
    "#     print(f\"Chunk {i+1} ({len(chunks[i])} characters):\")\n",
    "#     print(\"-\" * 50)\n",
    "#     print(chunks[i][:300] + \"...\")  # Show first 300 chars\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e8a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"chunk_preview.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for chunk in chunks[:3]:\n",
    "        f.write(chunk[:100] + \"...\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85565931",
   "metadata": {},
   "source": [
    "## 4. Question Answering with Ollama\n",
    "\n",
    "Now, let's implement the question answering functionality using the Ollama API with the deepseek-r1 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f9a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_ollama(query, context):\n",
    "    \"\"\"\n",
    "    Generate an answer using Ollama API based on the provided context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's question\n",
    "        context (str): Context information retrieved from the vector store\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated answer from the model\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Answer the question based on the following context:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=\"deepseek-r1\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"Error generating response: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4049c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Answer a question using RAG approach.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User's question\n",
    "        vector_store: FAISS vector store\n",
    "        k (int): Number of relevant chunks to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        str: Answer to the question\n",
    "    \"\"\"\n",
    "    print(f\"Searching for context relevant to: '{query}'\")\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    start_time = time.time()\n",
    "    docs = vector_store.similarity_search(query, k=k)\n",
    "    search_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Retrieved {len(docs)} relevant chunks (took {search_time:.2f} seconds)\")\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    context_length = len(context)\n",
    "    \n",
    "    print(f\"Sending prompt with context of {context_length} characters to Ollama...\")\n",
    "    \n",
    "    # Get answer from Ollama\n",
    "    start_time = time.time()\n",
    "    answer = ask_ollama(query, context)\n",
    "    generation_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Generated answer of {len(answer)} characters (took {generation_time:.2f} seconds)\")\n",
    "    \n",
    "    return answer, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e070d5d",
   "metadata": {},
   "source": [
    "Let's test our question answering function with a sample question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9265555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample question\n",
    "sample_question = \"What are the main financial highlights?\"\n",
    "answer, relevant_docs = answer_question(sample_question, vector_store)\n",
    "\n",
    "print(\"\\nQuestion:\", sample_question)\n",
    "print(\"\\nAnswer:\")\n",
    "print(\"-\" * 80)\n",
    "print(answer)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\nTop relevant context used:\")\n",
    "for i, doc in enumerate(relevant_docs[:2]):  # Show only first 2 contexts\n",
    "    print(f\"\\nContext chunk {i+1}:\")\n",
    "    print(\"...\" + doc.page_content[:300] + \"...\")  # First 300 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849c1d93",
   "metadata": {},
   "source": [
    "## 5. Interactive Demo\n",
    "\n",
    "Let's create an interactive demo using IPython widgets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53724400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_demo(vector_store):\n",
    "    \"\"\"Create an interactive demo for question answering\"\"\"\n",
    "    \n",
    "    # Create widgets\n",
    "    question_input = widgets.Text(\n",
    "        value='',\n",
    "        description='Question:',\n",
    "        placeholder='Type your question here...',\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "    \n",
    "    k_slider = widgets.IntSlider(\n",
    "        value=4,\n",
    "        min=1,\n",
    "        max=10,\n",
    "        step=1,\n",
    "        description='Context chunks:',\n",
    "        layout=widgets.Layout(width='50%')\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Submit button\n",
    "    button = widgets.Button(\n",
    "        description='Get Answer',\n",
    "        button_style='primary',\n",
    "        tooltip='Click to get the answer',\n",
    "        icon='search'\n",
    "    )\n",
    "    \n",
    "    def on_button_clicked(_):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            if question_input.value.strip():\n",
    "                print(f\"Question: {question_input.value}\")\n",
    "                print(\"Generating answer...\")\n",
    "                answer, docs = answer_question(question_input.value, vector_store, k=k_slider.value)\n",
    "                display(Markdown(\"### Answer:\"))\n",
    "                display(Markdown(answer))\n",
    "                \n",
    "                display(Markdown(\"### Sources:\"))\n",
    "                for i, doc in enumerate(docs):\n",
    "                    display(Markdown(f\"**Source {i+1}** (extract):\\n> {doc.page_content[:150]}...\"))\n",
    "            else:\n",
    "                print(\"Please enter a question.\")\n",
    "    \n",
    "    button.on_click(on_button_clicked)\n",
    "    \n",
    "    # Layout\n",
    "    return widgets.VBox([\n",
    "        widgets.HTML(\"<h3>RAG Question Answering System</h3>\"),\n",
    "        widgets.HBox([question_input, button]),\n",
    "        k_slider,\n",
    "        output\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display the interactive demo\n",
    "demo = create_interactive_demo(vector_store)\n",
    "display(demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be953a44",
   "metadata": {},
   "source": [
    "## 6. Error Handling and Improvements\n",
    "\n",
    "Let's enhance our system with better error handling and explore potential improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSystem:\n",
    "    \"\"\"A more robust implementation of our RAG system with error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"deepseek-r1\", \n",
    "                 embeddings_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "                 chunk_size=1000, chunk_overlap=200):\n",
    "        self.model_name = model_name\n",
    "        self.embeddings_model_name = embeddings_model\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.vector_store = None\n",
    "        self.chunks = []\n",
    "        self.pdf_text = \"\"\n",
    "        \n",
    "    def load_document(self, file_path):\n",
    "        \"\"\"Load document and handle errors\"\"\"\n",
    "        try:\n",
    "            if file_path.lower().endswith('.pdf'):\n",
    "                self.pdf_text = load_pdf(file_path)\n",
    "                return True, f\"Successfully loaded PDF with {len(self.pdf_text)} characters\"\n",
    "            else:\n",
    "                return False, \"Only PDF files are currently supported\"\n",
    "        except FileNotFoundError:\n",
    "            return False, f\"File not found: {file_path}\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Error loading document: {str(e)}\"\n",
    "    \n",
    "    def process_document(self):\n",
    "        \"\"\"Process the loaded document and create vector store\"\"\"\n",
    "        try:\n",
    "            if not self.pdf_text:\n",
    "                return False, \"No document loaded. Please load a document first.\"\n",
    "            \n",
    "            self.vector_store, self.chunks = create_vector_store(\n",
    "                self.pdf_text, self.chunk_size, self.chunk_overlap\n",
    "            )\n",
    "            return True, f\"Successfully processed document into {len(self.chunks)} chunks\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Error processing document: {str(e)}\"\n",
    "    \n",
    "    def answer_query(self, query, k=4):\n",
    "        \"\"\"Answer a query with improved error handling\"\"\"\n",
    "        try:\n",
    "            if not self.vector_store:\n",
    "                return False, \"Vector store not initialized. Process a document first.\"\n",
    "            \n",
    "            if not query.strip():\n",
    "                return False, \"Query cannot be empty\"\n",
    "            \n",
    "            answer, docs = answer_question(query, self.vector_store, k)\n",
    "            return True, {\"answer\": answer, \"sources\": docs}\n",
    "        except Exception as e:\n",
    "            return False, f\"Error answering query: {str(e)}\"\n",
    "    \n",
    "    def suggest_improvements(self):\n",
    "        \"\"\"Suggest possible improvements to the RAG system\"\"\"\n",
    "        improvements = [\n",
    "            \"**Implement document metadata** - Track source information for each chunk\",\n",
    "            \"**Add document pre-processing** - Clean and normalize text before chunking\",\n",
    "            \"**Experiment with different embedding models** - Try models like E5, BGE, or INSTRUCTOR\",\n",
    "            \"**Implement re-ranking** - Use a cross-encoder to re-rank the retrieved chunks\",\n",
    "            \"**Add chat history** - Maintain conversation context for follow-up questions\",\n",
    "            \"**Optimize chunking strategy** - Try semantic chunking instead of character-based\",\n",
    "            \"**Implement hybrid search** - Combine vector search with keyword-based search\",\n",
    "            \"**Add evaluation metrics** - Measure relevance and answer quality\"\n",
    "        ]\n",
    "        return improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and test our improved RAG system\n",
    "rag = RAGSystem()\n",
    "\n",
    "# Show potential improvements\n",
    "print(\"Potential improvements for the RAG system:\")\n",
    "for i, improvement in enumerate(rag.suggest_improvements(), 1):\n",
    "    print(f\"{i}. {improvement}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a506484a",
   "metadata": {},
   "source": [
    "### Summary and Conclusion\n",
    "\n",
    "In this notebook, we've built a complete Retrieval-Augmented Generation (RAG) system that:\n",
    "\n",
    "1. Loads and processes PDF documents\n",
    "2. Chunks text and creates vector embeddings\n",
    "3. Performs similarity search to find relevant context\n",
    "4. Uses Ollama with the deepseek-r1 model to generate answers\n",
    "5. Provides an interactive interface for querying the document\n",
    "\n",
    "This implementation demonstrates the core components of a RAG system, but there are many ways to improve it further, as outlined in the improvements section.\n",
    "\n",
    "Key takeaways:\n",
    "- RAG enhances LLM responses with relevant context from specific documents\n",
    "- The quality of chunking and embeddings directly impacts retrieval performance\n",
    "- Local models like those in Ollama provide privacy and control advantages\n",
    "- Error handling is crucial for production-ready RAG systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
