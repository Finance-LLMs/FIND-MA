{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ba0ca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deepsearch-toolkit in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: certifi<2025.0.0,>=2024.07.04 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from deepsearch-toolkit) (2024.12.14)\n",
      "Requirement already satisfied: docling-core<3.0.0,>=2.0.0 in c:\\users\\akshat\\appdata\\roaming\\python\\python312\\site-packages (from deepsearch-toolkit) (2.25.0)\n",
      "Requirement already satisfied: platformdirs<5.0.0,>=3.5.1 in c:\\users\\akshat\\appdata\\roaming\\python\\python312\\site-packages (from deepsearch-toolkit) (4.3.6)\n",
      "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from deepsearch-toolkit) (1.5.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from deepsearch-toolkit) (2.10.6)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\akshat\\appdata\\roaming\\python\\python312\\site-packages (from deepsearch-toolkit) (2.8.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in c:\\users\\akshat\\appdata\\roaming\\python\\python312\\site-packages (from deepsearch-toolkit) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from deepsearch-toolkit) (1.1.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.27.1 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from deepsearch-toolkit) (2.32.3)\n",
      "Requirement already satisfied: six<2.0.0,>=1.16.0 in c:\\users\\akshat\\appdata\\roaming\\python\\python312\\site-packages (from deepsearch-toolkit) (1.17.0)\n",
      "Requirement already satisfied: tabulate<1.0.0,>=0.8.9 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from deepsearch-toolkit) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.64.0 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from deepsearch-toolkit) (4.66.5)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9.0 in c:\\users\\akshat\\appdata\\roaming\\python\\python312\\site-packages (from typer[all]<1.0.0,>=0.9.0->deepsearch-toolkit) (0.12.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from deepsearch-toolkit) (2.3.0)\n",
      "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in c:\\users\\akshat\\appdata\\roaming\\python\\python312\\site-packages (from docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (1.1.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (4.23.0)\n",
      "Requirement already satisfied: latex2mathml<4.0.0,>=3.77.0 in c:\\users\\akshat\\appdata\\roaming\\python\\python312\\site-packages (from docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (3.77.0)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.0.0 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (11.0.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.1 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from pydantic<3.0.0,>=2.0.3->deepsearch-toolkit) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from pydantic<3.0.0,>=2.0.3->deepsearch-toolkit) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from requests<3.0.0,>=2.27.1->deepsearch-toolkit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from requests<3.0.0,>=2.27.1->deepsearch-toolkit) (3.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\akshat\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.64.0->deepsearch-toolkit) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from typer<1.0.0,>=0.9.0->typer[all]<1.0.0,>=0.9.0->deepsearch-toolkit) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from typer<1.0.0,>=0.9.0->typer[all]<1.0.0,>=0.9.0->deepsearch-toolkit) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from typer<1.0.0,>=0.9.0->typer[all]<1.0.0,>=0.9.0->deepsearch-toolkit) (13.9.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (25.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (0.24.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from pandas<3.0.0,>=2.1.4->docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (1.26.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from pandas<3.0.0,>=2.1.4->docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from pandas<3.0.0,>=2.1.4->docling-core<3.0.0,>=2.0.0->deepsearch-toolkit) (2025.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.9.0->typer[all]<1.0.0,>=0.9.0->deepsearch-toolkit) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\akshat\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.9.0->typer[all]<1.0.0,>=0.9.0->deepsearch-toolkit) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\akshat\\anaconda3\\envs\\cuda_env\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.9.0->typer[all]<1.0.0,>=0.9.0->deepsearch-toolkit) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: typer 0.12.5 does not provide the extra 'all'\n"
     ]
    }
   ],
   "source": [
    "%pip install deepsearch-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d36766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    AcceleratorDevice,\n",
    "    AcceleratorOptions,\n",
    "    PdfPipelineOptions,\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.models.ocr_mac_model import OcrMacOptions\n",
    "from docling.models.tesseract_ocr_cli_model import TesseractCliOcrOptions\n",
    "from docling.models.tesseract_ocr_model import TesseractOcrOptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0053ab9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Akshat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "313462dc765c4c35b07402d5151845ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshat\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Akshat\\.cache\\huggingface\\hub\\models--yiyanghkust--finbert-tone. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1303dc5bb9d54cc0b37695bc5c14db57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/533 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0627126a32574ba093abadaf873e0523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:37:14,747 - INFO - Sentiment Analyzer Agent initialized with model: yiyanghkust/finbert-tone\n",
      "2025-04-10 12:37:16,186 - INFO - Going to convert document batch...\n",
      "2025-04-10 12:37:16,188 - INFO - Initializing pipeline for StandardPdfPipeline with options hash a13a654fe19e39b385cda0cb5fed5bb9\n",
      "2025-04-10 12:37:16,232 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-04-10 12:37:16,233 - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-04-10 12:37:16,464 - INFO - Accelerator device: 'cuda:0'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c511d096bb45c8bfb461cccf5cf997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/439M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-10 12:37:18,296 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-04-10 12:37:19,361 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-04-10 12:37:19,906 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-04-10 12:37:19,907 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-04-10 12:37:19,908 - INFO - Processing document annual-report-2024.pdf\n",
      "2025-04-10 12:39:10,223 - WARNING - Encountered an error during conversion of document 2528d97fd226103dd31caed537ab4637336858fe7216d2789173a8c2efbea22a:\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"C:\\Users\\Akshat\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\pipeline\\base_pipeline.py\", line 163, in _build_document\n",
      "    for p in pipeline_pages:  # Must exhaust!\n",
      "             ^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"C:\\Users\\Akshat\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\pipeline\\base_pipeline.py\", line 127, in _apply_on_pages\n",
      "    yield from page_batch\n",
      "\n",
      "  File \"C:\\Users\\Akshat\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\models\\page_assemble_model.py\", line 68, in __call__\n",
      "    for page in page_batch:\n",
      "                ^^^^^^^^^^\n",
      "\n",
      "  File \"C:\\Users\\Akshat\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\models\\table_structure_model.py\", line 257, in __call__\n",
      "    tf_output = self.tf_predictor.multi_table_predict(\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"C:\\Users\\Akshat\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling_ibm_models\\tableformer\\data_management\\tf_predictor.py\", line 485, in multi_table_predict\n",
      "    tf_responses, predict_details = self.predict(\n",
      "                                    ^^^^^^^^^^^^^\n",
      "\n",
      "  File \"C:\\Users\\Akshat\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling_ibm_models\\tableformer\\data_management\\tf_predictor.py\", line 731, in predict\n",
      "    image_batch = self._prepare_image(table_image)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"C:\\Users\\Akshat\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling_ibm_models\\tableformer\\data_management\\tf_predictor.py\", line 1008, in _prepare_image\n",
      "    img, _ = normalize(mat_image, None)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"C:\\Users\\Akshat\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling_ibm_models\\tableformer\\data_management\\transforms.py\", line 37, in __call__\n",
      "    return F.normalize(tensor, self.mean, self.std), target\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n",
      "  File \"C:\\Users\\Akshat\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling_ibm_models\\tableformer\\data_management\\functional.py\", line 53, in normalize\n",
      "    return (tensor.astype(np.float32) - 255.0 * np.array(mean)) / np.array(std)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~\n",
      "\n",
      "numpy.core._exceptions._ArrayMemoryError: Unable to allocate 20.1 MiB for an array with shape (801, 1094, 3) and data type float64\n",
      "\n",
      "2025-04-10 12:39:10,367 - ERROR - Failed to process PDF with Deep Search pipeline: Unable to allocate 20.1 MiB for an array with shape (801, 1094, 3) and data type float64\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 20.1 MiB for an array with shape (801, 1094, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 239\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Use real PDF path here\u001b[39;00m\n\u001b[0;32m    238\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannual-report-2024.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 239\u001b[0m pdf_text \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_text_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m result \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mrun(pdf_text, source_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    242\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment analysis completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 59\u001b[0m, in \u001b[0;36mSentimentAnalyzerAgent._extract_text_from_pdf\u001b[1;34m(self, pdf_path)\u001b[0m\n\u001b[0;32m     52\u001b[0m doc_converter \u001b[38;5;241m=\u001b[39m DocumentConverter(\n\u001b[0;32m     53\u001b[0m     format_options\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m     54\u001b[0m         InputFormat\u001b[38;5;241m.\u001b[39mPDF: PdfFormatOption(pipeline_options\u001b[38;5;241m=\u001b[39mpipeline_options)\n\u001b[0;32m     55\u001b[0m     }\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Convert document\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m conv_result \u001b[38;5;241m=\u001b[39m \u001b[43mdoc_converter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Define output directory\u001b[39;00m\n\u001b[0;32m     62\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscratch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\pydantic\\_internal\\_validate_call.py:38\u001b[0m, in \u001b[0;36mupdate_wrapper_attributes.<locals>.wrapper_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(wrapped)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\anaconda3\\envs\\cuda_env\\Lib\\site-packages\\pydantic\\_internal\\_validate_call.py:111\u001b[0m, in \u001b[0;36mValidateCallWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m--> 111\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpydantic_core\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mArgsKwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__:\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__return_pydantic_validator__(res)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\document_converter.py:220\u001b[0m, in \u001b[0;36mDocumentConverter.convert\u001b[1;34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;129m@validate_call\u001b[39m(config\u001b[38;5;241m=\u001b[39mConfigDict(strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m     page_range: PageRange \u001b[38;5;241m=\u001b[39m DEFAULT_PAGE_RANGE,\n\u001b[0;32m    211\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ConversionResult:\n\u001b[0;32m    212\u001b[0m     all_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_all(\n\u001b[0;32m    213\u001b[0m         source\u001b[38;5;241m=\u001b[39m[source],\n\u001b[0;32m    214\u001b[0m         raises_on_error\u001b[38;5;241m=\u001b[39mraises_on_error,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m         page_range\u001b[38;5;241m=\u001b[39mpage_range,\n\u001b[0;32m    219\u001b[0m     )\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mall_res\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\document_converter.py:243\u001b[0m, in \u001b[0;36mDocumentConverter.convert_all\u001b[1;34m(self, source, headers, raises_on_error, max_num_pages, max_file_size, page_range)\u001b[0m\n\u001b[0;32m    240\u001b[0m conv_res_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert(conv_input, raises_on_error\u001b[38;5;241m=\u001b[39mraises_on_error)\n\u001b[0;32m    242\u001b[0m had_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_res\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_res_iter\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhad_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_res\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mConversionStatus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSUCCESS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mConversionStatus\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPARTIAL_SUCCESS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\document_converter.py:278\u001b[0m, in \u001b[0;36mDocumentConverter._convert\u001b[1;34m(self, conv_input, raises_on_error)\u001b[0m\n\u001b[0;32m    269\u001b[0m _log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoing to convert document batch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    271\u001b[0m \u001b[38;5;66;03m# parallel processing only within input_batch\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# with ThreadPoolExecutor(\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;66;03m#    max_workers=settings.perf.doc_batch_concurrency\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# ) as pool:\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m#   yield from pool.map(self.process_document, input_batch)\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# Note: PDF backends are not thread-safe, thread pool usage was disabled.\u001b[39;00m\n\u001b[1;32m--> 278\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_document\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraises_on_error\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43melapsed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\document_converter.py:324\u001b[0m, in \u001b[0;36mDocumentConverter._process_document\u001b[1;34m(self, in_doc, raises_on_error)\u001b[0m\n\u001b[0;32m    320\u001b[0m valid \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_formats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m in_doc\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_formats\n\u001b[0;32m    322\u001b[0m )\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[1;32m--> 324\u001b[0m     conv_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraises_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     error_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not allowed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_doc\u001b[38;5;241m.\u001b[39mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\document_converter.py:347\u001b[0m, in \u001b[0;36mDocumentConverter._execute_pipeline\u001b[1;34m(self, in_doc, raises_on_error)\u001b[0m\n\u001b[0;32m    345\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_pipeline(in_doc\u001b[38;5;241m.\u001b[39mformat)\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipeline \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 347\u001b[0m     conv_res \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_doc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraises_on_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraises_on_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raises_on_error:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\pipeline\\base_pipeline.py:53\u001b[0m, in \u001b[0;36mBasePipeline.execute\u001b[1;34m(self, in_doc, raises_on_error)\u001b[0m\n\u001b[0;32m     51\u001b[0m     conv_res\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m ConversionStatus\u001b[38;5;241m.\u001b[39mFAILURE\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m raises_on_error:\n\u001b[1;32m---> 53\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unload(conv_res)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\pipeline\\base_pipeline.py:45\u001b[0m, in \u001b[0;36mBasePipeline.execute\u001b[1;34m(self, in_doc, raises_on_error)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m TimeRecorder(\n\u001b[0;32m     41\u001b[0m         conv_res, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline_total\u001b[39m\u001b[38;5;124m\"\u001b[39m, scope\u001b[38;5;241m=\u001b[39mProfilingScope\u001b[38;5;241m.\u001b[39mDOCUMENT\n\u001b[0;32m     42\u001b[0m     ):\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;66;03m# These steps are building and assembling the structure of the\u001b[39;00m\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;66;03m# output DoclingDocument.\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m         conv_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m         conv_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assemble_document(conv_res)\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;66;03m# From this stage, all operations should rely only on conv_res.output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\pipeline\\base_pipeline.py:198\u001b[0m, in \u001b[0;36mPaginatedPipeline._build_document\u001b[1;34m(self, conv_res)\u001b[0m\n\u001b[0;32m    191\u001b[0m         trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    192\u001b[0m             traceback\u001b[38;5;241m.\u001b[39mformat_exception(\u001b[38;5;28mtype\u001b[39m(e), e, e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    193\u001b[0m         )\n\u001b[0;32m    194\u001b[0m         _log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    195\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered an error during conversion of document \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconv_res\u001b[38;5;241m.\u001b[39minput\u001b[38;5;241m.\u001b[39mdocument_hash\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m         )\n\u001b[1;32m--> 198\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conv_res\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\pipeline\\base_pipeline.py:163\u001b[0m, in \u001b[0;36mPaginatedPipeline._build_document\u001b[1;34m(self, conv_res)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# 2. Run pipeline stages\u001b[39;00m\n\u001b[0;32m    161\u001b[0m pipeline_pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_on_pages(conv_res, init_pages)\n\u001b[1;32m--> 163\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpipeline_pages\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Must exhaust!\u001b[39;49;00m\n\u001b[0;32m    164\u001b[0m \n\u001b[0;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Cleanup cached images\u001b[39;49;00m\n\u001b[0;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeep_images\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_image_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\pipeline\\base_pipeline.py:127\u001b[0m, in \u001b[0;36mPaginatedPipeline._apply_on_pages\u001b[1;34m(self, conv_res, page_batch)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_pipe:\n\u001b[0;32m    125\u001b[0m     page_batch \u001b[38;5;241m=\u001b[39m model(conv_res, page_batch)\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m page_batch\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\models\\page_assemble_model.py:68\u001b[0m, in \u001b[0;36mPageAssembleModel.__call__\u001b[1;34m(self, conv_res, page_batch)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m, conv_res: ConversionResult, page_batch: Iterable[Page]\n\u001b[0;32m     67\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterable[Page]:\n\u001b[1;32m---> 68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpage_batch\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01massert\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling\\models\\table_structure_model.py:257\u001b[0m, in \u001b[0;36mTableStructureModel.__call__\u001b[1;34m(self, conv_res, page_batch)\u001b[0m\n\u001b[0;32m    248\u001b[0m         tokens\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m    249\u001b[0m             {\n\u001b[0;32m    250\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: new_cell\u001b[38;5;241m.\u001b[39mindex,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m             }\n\u001b[0;32m    254\u001b[0m         )\n\u001b[0;32m    255\u001b[0m page_input[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokens\n\u001b[1;32m--> 257\u001b[0m tf_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_table_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtbl_box\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_matching\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_cell_matching\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m table_out \u001b[38;5;241m=\u001b[39m tf_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    261\u001b[0m table_cells \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling_ibm_models\\tableformer\\data_management\\tf_predictor.py:485\u001b[0m, in \u001b[0;36mTFPredictor.multi_table_predict\u001b[1;34m(self, iocr_page, table_bboxes, do_matching, correct_overlapping_cells, sort_row_col_indexes)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# table_image = page_image\u001b[39;00m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_matching:\n\u001b[1;32m--> 485\u001b[0m     tf_responses, predict_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43miocr_page\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable_bbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorrect_overlapping_cells\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m     tf_responses, predict_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_dummy(\n\u001b[0;32m    495\u001b[0m         iocr_page, table_bbox, table_image, scale_factor, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling_ibm_models\\tableformer\\data_management\\tf_predictor.py:731\u001b[0m, in \u001b[0;36mTFPredictor.predict\u001b[1;34m(self, iocr_page, table_bbox, table_image, scale_factor, eval_res_preds, correct_overlapping_cells)\u001b[0m\n\u001b[0;32m    729\u001b[0m max_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    730\u001b[0m beam_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeam_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 731\u001b[0m image_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    732\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m    733\u001b[0m prediction \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling_ibm_models\\tableformer\\data_management\\tf_predictor.py:1008\u001b[0m, in \u001b[0;36mTFPredictor._prepare_image\u001b[1;34m(self, mat_image)\u001b[0m\n\u001b[0;32m   1005\u001b[0m resized_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresized_image\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1006\u001b[0m resize \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mResize([resized_size, resized_size])\n\u001b[1;32m-> 1008\u001b[0m img, _ \u001b[38;5;241m=\u001b[39m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m img, _ \u001b[38;5;241m=\u001b[39m resize(img, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (channels, width, height)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling_ibm_models\\tableformer\\data_management\\transforms.py:37\u001b[0m, in \u001b[0;36mNormalize.__call__\u001b[1;34m(self, tensor, target)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor, target\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m, target\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\docling_ibm_models\\tableformer\\data_management\\functional.py:53\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_numpy_image(tensor):\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndefined type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 20.1 MiB for an array with shape (801, 1094, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from typing import List, Dict, Optional\n",
    "import textwrap\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SentimentAnalyzerAgent:\n",
    "    \"\"\"An agent that performs sentiment analysis on financial texts using FinBERT.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"yiyanghkust/finbert-tone\", max_length: int = 512):\n",
    "        \"\"\"Initialize the agent with a model and tokenizer.\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer, self.model = self._load_finbert()\n",
    "        logger.info(f\"Sentiment Analyzer Agent initialized with model: {model_name}\")\n",
    "\n",
    "    def _load_finbert(self) -> tuple:\n",
    "        \"\"\"Load the FinBERT model and tokenizer.\"\"\"\n",
    "        try:\n",
    "            tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "            model = BertForSequenceClassification.from_pretrained(self.model_name)\n",
    "            return tokenizer, model\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load FinBERT: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _extract_text_from_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF using Deep Search OCR pipeline.\"\"\"\n",
    "        try:\n",
    "            # Configure pipeline options\n",
    "            pipeline_options = PdfPipelineOptions()\n",
    "            pipeline_options.do_ocr = True\n",
    "            pipeline_options.do_table_structure = True\n",
    "            pipeline_options.table_structure_options.do_cell_matching = True\n",
    "            pipeline_options.ocr_options.lang = [\"es\"]\n",
    "            pipeline_options.accelerator_options = AcceleratorOptions(\n",
    "                num_threads=4, device=AcceleratorDevice.AUTO\n",
    "            )\n",
    "\n",
    "            doc_converter = DocumentConverter(\n",
    "                format_options={\n",
    "                    InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Convert document\n",
    "            conv_result = doc_converter.convert(pdf_path)\n",
    "\n",
    "            # Define output directory\n",
    "            output_dir = Path(\"scratch\")\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            doc_filename = conv_result.input.file.stem\n",
    "\n",
    "            # Export text version and read it\n",
    "            text_output_path = output_dir / f\"{doc_filename}.txt\"\n",
    "            with text_output_path.open(\"w\", encoding=\"utf-8\") as fp:\n",
    "                fp.write(conv_result.document.export_to_text())\n",
    "\n",
    "            # Read and return text content\n",
    "            with text_output_path.open(\"r\", encoding=\"utf-8\") as fp:\n",
    "                return fp.read().strip()\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to process PDF with Deep Search pipeline: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def _extract_text_from_pdf_like_content(self, pdf_text: str) -> str:\n",
    "        \"\"\"Extract and clean text from PDF-like OCR-tagged content.\"\"\"\n",
    "        content_blocks = re.findall(r'<CONTENT_FROM_OCR>(.*?)</CONTENT_FROM_OCR>', pdf_text, re.DOTALL)\n",
    "        full_text = \"\"\n",
    "        for block in content_blocks:\n",
    "            if any(keyword in block.lower() for keyword in [\"safe harbor\", \"e-voting\", \"instructions\", \"notice of the\"]):\n",
    "                continue\n",
    "            cleaned_block = re.sub(r'(the red box\\s+)+', ' ', block.strip())\n",
    "            full_text += cleaned_block + \"\\n\\n\"\n",
    "        return full_text.strip()\n",
    "\n",
    "    def _preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Split text into chunks using sentence-aware token-based logic.\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        chunks, current_chunk = [], []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            tokens = self.tokenizer.encode(' '.join(current_chunk + [sentence]), add_special_tokens=False)\n",
    "            if len(tokens) <= self.max_length:\n",
    "                current_chunk.append(sentence)\n",
    "            else:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def _analyze_sentiment(self, chunks: List[str]) -> List[Dict]:\n",
    "        \"\"\"Run sentiment analysis on text chunks.\"\"\"\n",
    "        self.model.eval()\n",
    "        sentiment_results = []\n",
    "        label_map = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for chunk in chunks:\n",
    "                inputs = self.tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True, max_length=self.max_length)\n",
    "                outputs = self.model(**inputs)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.softmax(logits, dim=1).tolist()[0]\n",
    "\n",
    "                pred_label_idx = torch.argmax(logits, dim=1).item()\n",
    "                pred_label = label_map[pred_label_idx]\n",
    "                confidence = probs[pred_label_idx]\n",
    "\n",
    "                sentiment_results.append({\n",
    "                    \"text\": chunk,\n",
    "                    \"sentiment\": pred_label,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"probabilities\": dict(zip([\"negative\", \"neutral\", \"positive\"], probs))\n",
    "                })\n",
    "\n",
    "        return sentiment_results\n",
    "\n",
    "    def _aggregate_sentiment(self, sentiment_results: List[Dict]) -> Dict:\n",
    "        \"\"\"Aggregate sentiment across chunks.\"\"\"\n",
    "        avg_probs = {\"negative\": 0, \"neutral\": 0, \"positive\": 0}\n",
    "        n_chunks = len(sentiment_results)\n",
    "\n",
    "        for result in sentiment_results:\n",
    "            for label, prob in result[\"probabilities\"].items():\n",
    "                avg_probs[label] += prob / n_chunks\n",
    "\n",
    "        overall_sentiment = max(avg_probs, key=avg_probs.get)\n",
    "        overall_confidence = avg_probs[overall_sentiment]\n",
    "\n",
    "        return {\n",
    "            \"overall_sentiment\": overall_sentiment,\n",
    "            \"overall_confidence\": overall_confidence,\n",
    "            \"average_probabilities\": avg_probs,\n",
    "            \"detailed_results\": sentiment_results\n",
    "        }\n",
    "\n",
    "    def export_to_csv(self, results: List[Dict], filename: str = \"sentiment_output.csv\"):\n",
    "        \"\"\"Export chunk-level sentiment results to a CSV file.\"\"\"\n",
    "        rows = []\n",
    "        for i, r in enumerate(results, 1):\n",
    "            rows.append({\n",
    "                \"Chunk No.\": i,\n",
    "                \"Sentiment\": r[\"sentiment\"],\n",
    "                \"Confidence\": r[\"confidence\"],\n",
    "                \"Negative\": r[\"probabilities\"][\"negative\"],\n",
    "                \"Neutral\": r[\"probabilities\"][\"neutral\"],\n",
    "                \"Positive\": r[\"probabilities\"][\"positive\"],\n",
    "                \"Text\": r[\"text\"]\n",
    "            })\n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(filename, index=False)\n",
    "        logger.info(f\"Results exported to {os.path.abspath(filename)}\")\n",
    "\n",
    "    def visualize_sentiments(self, results: List[Dict]):\n",
    "        \"\"\"Generate and display sentiment distribution pie and bar charts.\"\"\"\n",
    "        counts = {\"positive\": 0, \"neutral\": 0, \"negative\": 0}\n",
    "        for r in results:\n",
    "            counts[r[\"sentiment\"]] += 1\n",
    "\n",
    "        labels = list(counts.keys())\n",
    "        values = list(counts.values())\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.pie(values, labels=labels, autopct=\"%1.1f%%\", startangle=140)\n",
    "        plt.title(\"Sentiment Distribution (Pie Chart)\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(labels, values)\n",
    "        plt.title(\"Sentiment Count per Label (Bar Chart)\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xlabel(\"Sentiment\")\n",
    "        plt.show()\n",
    "\n",
    "    def show_top_negative_chunks(self, results: List[Dict], top_n: int = 5):\n",
    "        \"\"\"Print top N chunks with highest negative sentiment.\"\"\"\n",
    "        sorted_res = sorted(results, key=lambda x: x[\"probabilities\"][\"negative\"], reverse=True)[:top_n]\n",
    "        print(\"\\n=== Top Negative Chunks ===\")\n",
    "        for i, res in enumerate(sorted_res, 1):\n",
    "            print(f\"Chunk {i} - Neg Score: {res['probabilities']['negative']:.2f}\")\n",
    "            print(textwrap.shorten(res[\"text\"], width=120, placeholder=\"...\"))\n",
    "            print()\n",
    "\n",
    "    def run(self, input_data: str, source_type: str = \"pdf\") -> Dict:\n",
    "        \"\"\"\n",
    "        Process input text and return sentiment analysis results.\n",
    "        Args:\n",
    "            input_data: Raw text (e.g., PDF content or OCR tagged).\n",
    "            source_type: Type of input ('pdf', 'text', etc.).\n",
    "        Returns:\n",
    "            Dict with sentiment analysis results.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Processing input data (source: {source_type})...\")\n",
    "\n",
    "        if source_type == \"pdf\":\n",
    "            processed_text = self._extract_text_from_pdf_like_content(input_data)\n",
    "        else:\n",
    "            processed_text = input_data\n",
    "\n",
    "        logger.info(f\"Extracted text length: {len(processed_text)} characters\")\n",
    "\n",
    "        chunks = self._preprocess_text(processed_text)\n",
    "        logger.info(f\"Split into {len(chunks)} chunks\")\n",
    "\n",
    "        sentiment_results = self._analyze_sentiment(chunks)\n",
    "        logger.info(\"Sentiment analysis completed\")\n",
    "\n",
    "        aggregated_result = self._aggregate_sentiment(sentiment_results)\n",
    "        logger.info(f\"Overall sentiment: {aggregated_result['overall_sentiment']} (Confidence: {aggregated_result['overall_confidence']:.2f})\")\n",
    "\n",
    "        return {\n",
    "            \"agent\": \"SentimentAnalyzer\",\n",
    "            \"source_type\": source_type,\n",
    "            \"result\": aggregated_result\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = SentimentAnalyzerAgent()\n",
    "\n",
    "    # Use real PDF path here\n",
    "    pdf_path = \"annual-report-2024.pdf\"\n",
    "    pdf_text = agent._extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    result = agent.run(pdf_text, source_type=\"text\")\n",
    "    logger.info(\"Sentiment analysis completed.\")\n",
    "\n",
    "    print(\"\\n=== Sentiment Analyzer Agent Output ===\")\n",
    "    print(json.dumps(result, indent=2, default=str))\n",
    "\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"Overall Sentiment: {result['result']['overall_sentiment'].capitalize()} (Confidence: {result['result']['overall_confidence']:.2f})\")\n",
    "    print(\"Average Probabilities:\")\n",
    "    for label, prob in result['result']['average_probabilities'].items():\n",
    "        print(f\"  {label.capitalize()}: {prob:.2f}\")\n",
    "\n",
    "    print(\"\\nDetailed Results (First 3 Chunks):\")\n",
    "    for i, detail in enumerate(result['result']['detailed_results'][:3], 1):\n",
    "        print(f\"Chunk {i}:\")\n",
    "        print(f\"  Text: {textwrap.shorten(detail['text'], width=50, placeholder='...')}\")\n",
    "        print(f\"  Sentiment: {detail['sentiment'].capitalize()} (Confidence: {detail['confidence']:.2f})\")\n",
    "\n",
    "    agent.export_to_csv(result['result']['detailed_results'])\n",
    "    agent.visualize_sentiments(result['result']['detailed_results'])\n",
    "    agent.show_top_negative_chunks(result['result']['detailed_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3cbd9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
